{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bgtfN1Dcz6qn"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "with open('../gpt_key.txt', 'r') as file:\n",
        "    openai.api_key = file.read().rstrip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt bank\n",
        "Here we can create different ways of prompting GPT3 with medical text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jyc1o8MbzaVA"
      },
      "outputs": [],
      "source": [
        "prompt_bank = {\n",
        "      \"Prompt 'something'\": \"something\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Medical Data CSV as Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('...', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Temperature\n",
        "So, temperature ranges from 0.0-1.0, where 0.0 has no randomness, and 1.0 allows GPT-3 to be very \"creative.\" Temperature=0.0 will give the same exact result each time. For most natural language generation tasks, people set temperature at ~0.7. We can set it at 0.3 (which is good at just giving binary answers), and maybe also try values of 0.5, and 0.7 for comparison (which will probably involve us parsing some responses)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interact with GPT3 (binary true or false)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def a_prompt(phrase, prompt_type):\n",
        "    return f\"\"\"True or False, the phrase '{phrase}' is {prompt_type}?\n",
        "    Answer:\"\"\"\n",
        "\n",
        "def parse_boolean_text(t):\n",
        "  t = t.lower().split(' ')\n",
        "  for val in ['true', 'false', 'yes', 'no', 'true.', 'false.', 'yes.', 'no.']:\n",
        "    if val in t:\n",
        "      return int(val == 'true') or int(val == 'yes')\n",
        "  \n",
        "  # Special case: sometimes GPT3 returns a negation.\n",
        "  # this might be a bit of a hacky fix...\n",
        "  if 'not' in t:\n",
        "    return 0\n",
        "\n",
        "  return -1\n",
        "\n",
        "def score_row(row, prompt_type):\n",
        "    response = openai.Completion.create(\n",
        "                engine=\"text-davinci-001\",\n",
        "                prompt=a_prompt(row['text'], prompt_type),\n",
        "                temperature=temperature,\n",
        "              )\n",
        "    return parse_boolean_text(response[\"choices\"][0][\"text\"])\n",
        "\n",
        "for key, prompt_type in prompt_bank.items():\n",
        "  data[key] = data.apply(lambda row: score_row(row, prompt_type=prompt_type), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = data.iloc[7]['text']\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO8_pKvdzwch",
        "outputId": "36c5e7b4-13a1-4e74-aef4-21f6051a67e0"
      },
      "outputs": [],
      "source": [
        "phrase = p\n",
        "print(\"Phrase: \" + phrase)\n",
        "print()\n",
        "for p_t, prompt in prompt_bank.items():\n",
        "  response = openai.Completion.create(\n",
        "              engine=\"text-davinci-001\",\n",
        "              prompt=a_prompt(phrase, prompt),\n",
        "              temperature=temperature,\n",
        "            )\n",
        "  print(\"For prompt: \" + prompt)\n",
        "  print(response[\"choices\"][0][\"text\"])\n",
        "  print(parse_boolean_text(response[\"choices\"][0][\"text\"]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "GPT-3 Prompts for Toxicity",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
